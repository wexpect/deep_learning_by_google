{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hIbr52I7Z7U"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 1\n",
    "------------\n",
    "\n",
    "The objective of this assignment is to learn about simple data curation practices, and familiarize you with some of the data we'll be reusing later.\n",
    "\n",
    "This notebook uses the [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset to be used with python experiments. This dataset is designed to look like the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "apJbCsBHl-2A"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNWGtZaXn-5j"
   },
   "source": [
    "First, we'll download the dataset to our local machine. The data consists of characters rendered in a variety of fonts on a 28x28 image. The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labeled examples (50k and 1.9k for each class). Given these sizes, it should be possible to train models quickly on any machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 186058,
     "status": "ok",
     "timestamp": 1444485672507,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "EYRJ4ICW6-da",
    "outputId": "0d0f85df-155f-4a89-8e7e-ee32df36ec8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ./notMNIST_large.tar.gz\n",
      "Found and verified ./notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cC3p0oEyF8QT"
   },
   "source": [
    "Extract the dataset from the compressed .tar.gz file.\n",
    "This should give you a set of directories, labeled A through J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 186055,
     "status": "ok",
     "timestamp": 1444485672525,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "H8CBE-WZ8nmj",
    "outputId": "ef6c790c-2513-4b09-962e-27c79390c762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./notMNIST_large already present - Skipping extraction of ./notMNIST_large.tar.gz.\n",
      "['./notMNIST_large/A', './notMNIST_large/B', './notMNIST_large/C', './notMNIST_large/D', './notMNIST_large/E', './notMNIST_large/F', './notMNIST_large/G', './notMNIST_large/H', './notMNIST_large/I', './notMNIST_large/J']\n",
      "./notMNIST_small already present - Skipping extraction of ./notMNIST_small.tar.gz.\n",
      "['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4riXK3IoHgx6"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Let's take a peek at some of the data to make sure it looks sensible. Each exemplar should be an image of a character A through J rendered in a different font. Display a sample of the images that we just downloaded. Hint: you can use the package IPython.display.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB1ElEQVR4nF2SvWuUQRDGn919TyTi\noRYR/IigxQVBMYRINCJ+FFqIQvAPsAmWkYCojUUgiJWWKkgsY2FzkDRpRIyVpNLTgIoIkRRGJHqn\nuXd3fhbvHbnLdDs/ZmeemUeSJLksaOv4EnMlOW0O53RuEYvcULYZaeeVOUhm/Dgo3w29ZogpAZFp\nhU2F27/TpBUnumnQSJE3IzLb/W+mW+QFhcRoV6lTlQiJ6icsUSt3yHEqrxQ1Z+4RybmvUkfL85hh\ncPQYhpEuKMt8u+VdcjDjrD6QMJaPyykEeSnporzkkoa0IOTY83p+rC8lSV6HGxgA6z9/F0MnqFdP\nSSppoiWkI2I0Gofk5N6S2kuwDcy7bfK7p2inzDZwZEL6ut5mKQKQR4DEl7L8cmzJxYX6r0ZU1lre\n5FqQ+l6RgMTipcqusKN/9EXxPK2gkkaK3X3bp+urT68N9w/XSESeKyjoJAY5T+Qr0ytYocPIB6Sg\ncSJEbiuT/JGbL+utaZ9JmWaKe11V8JkkHRib/QdGY0DSG3KzJkMKkvOZU9AdciKPpZ6PJBLve9on\n9lu0dxVLzEvu8qPa3z8Lgx3WCXpIMzJZXHR/r+90ulPvZ1irSAqZJN9lOa/BJR7oP+2Ve278xbrW\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABUElEQVR4nH2SvUpDQRCFz+yuIgQk\nYKONSSmpJBGsrSxT2dn4AGKK9EI6sdTGKo3PIKTxDSSdjY2kFWIZcvfnWNydm2uCftWw3w47e3YB\nAA79RWRFXPThkLHozlmXnHdh1bVmdUdGzlqlNdKcMvAXgdOmGECMm9BzDc+JMwKH8aYjPcdwwIie\nZIo1EknPETBYP2917kBICCiX7zbqzWLnmQJCQJJM7KBGh4kkqUk0TK2zkSuVKUnKpSStDP5BO0VE\nVqVKllXBoPsDCpVS2oOv1UBhX/fdlSEUS6VY+nwTAk8b0Rb3eQXWvtCTSYmBF+3cCiO7b7V4U+A1\nTlXC4PCz+gjJcwjTqyQsjr9T1L5bbKMm4XCeB4y8g7XohVhJOFzl+R7gxOKE9InM+ZjU3qIA+BBQ\nsDO82UO0GoWmqbQfF6RGBlOiCQJH44J/PJaxwNnrD42Fhlk7sTnAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACM0lEQVR4nG2SXWjOYRjGf8/zf/ba\nV9SUaGU+J6a3TCk0pYgDLNq5UkQO1E4UpcSZWlFOVhw4cajkYEloUmrYDGlrpjFjwmxj4/3fz3M5\neEcOXHXfJ9f9cd0f8C88zS+HRtMpAsCcd1mZDFZcB7wPAUBlUlYmjaXKg+8x+5vptGRbgpicjy2W\nYt7S6BP4b+WAdv0PfeWyxTx6f/15IZFJMJE7zatyvQ5w7vGG6LV28I/oVXXJv/swp3/JtKI+Lqi8\n1nWrq61QWf1Z0qVCdQZkbFcy3WWNJO2HxVP5bL5rbjjalZs6741bntvYyMh7SRobuV8IIDaC583u\nOu9I8x0VgGqz1yXA+X6ZrBg69Usva2orT+uX3lRlnoBP9SuAt4O2Hnj2A5YhhmaBgGNtTRIvfi5c\nGV0sdmZxV1QccJkRcGwgoacUF5HR1ASQMSAgIJrBux76N8mRIgSXsmEfAUfNiEzf6/+9q5uzjJ2K\nebrDmXzy68TExKyl6a+TfUfqyiu4rNx0lHU7eiTpZsf5J5I0fhgPlcMyzSwFGidjKd3mwCezkrob\nvYcWJUu38b7AoTxKD6UkXSw3vaDcdIxAoGpYMUpJkydwHii8kGmmAR9oemQpScn0gAoPsDXKdAcP\nbV8kU5xRijpZfsuOmJuOExacjTLTVOt+WYylLXhgVLKfy2GPlEf1NcM5WUnjDc7B5isfdBfv2Tsm\nXa0lZO6GpFetZACNHQfJcKzuPgMex8Le6/sqcPAbQZlTNwH1DFwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(filename='./notMNIST_large/A/emlwZXJoZWFkLnR0Zg==.png'))\n",
    "display(Image(filename='./notMNIST_large/A/a29ydW5pc2hpLnR0Zg==.png'))\n",
    "display(Image(filename='./notMNIST_large/A/a2FkZW4udHRm.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBdkjESPK8tw"
   },
   "source": [
    "Now let's load the data in a more manageable format. Since, depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate dataset, store them on disk and curate them independently. Later we'll merge them into a single dataset of manageable size.\n",
    "\n",
    "We'll convert the entire dataset into a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road. \n",
    "\n",
    "A few images might not be readable, we'll just skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 30
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 399874,
     "status": "ok",
     "timestamp": 1444485886378,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "h7q0XhG3MJdf",
    "outputId": "92c391bb-86ff-431d-9ada-315568a19e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./notMNIST_large/A.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/B.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/C.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/D.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/E.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/F.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/G.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/H.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/I.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/J.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/A.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/B.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/C.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/D.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/E.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/F.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/G.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/H.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/I.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/J.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(\n",
    "      shape=(len(image_files), image_size, image_size),\n",
    "      dtype=np.float32\n",
    "  )\n",
    "\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      # ndimage.read read data as [0, 255], need to scale it to [-1, 1]\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./notMNIST_large/A.pickle', './notMNIST_large/B.pickle', './notMNIST_large/C.pickle', './notMNIST_large/D.pickle', './notMNIST_large/E.pickle', './notMNIST_large/F.pickle', './notMNIST_large/G.pickle', './notMNIST_large/H.pickle', './notMNIST_large/I.pickle', './notMNIST_large/J.pickle']\n",
      "['./notMNIST_small/A.pickle', './notMNIST_small/B.pickle', './notMNIST_small/C.pickle', './notMNIST_small/D.pickle', './notMNIST_small/E.pickle', './notMNIST_small/F.pickle', './notMNIST_small/G.pickle', './notMNIST_small/H.pickle', './notMNIST_small/I.pickle', './notMNIST_small/J.pickle']\n"
     ]
    }
   ],
   "source": [
    "print(train_datasets)\n",
    "print(test_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUdbskYE2d87"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Let's verify that the data still looks good. Displaying a sample of the labels and images from the ndarray. Hint: you can use matplotlib.pyplot.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52909, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(train_datasets[0], 'rb') as f:\n",
    "    dataset_tmp = pickle.load(f)\n",
    "dataset_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tmp = dataset_tmp[0]\n",
    "image_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5       , -0.5       , -0.5       , -0.5       , -0.48431373,\n",
       "        -0.5       , -0.19019608,  0.46862745,  0.49215686,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.49215686,\n",
       "         0.46862745, -0.19019608, -0.5       , -0.48431373, -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       ],\n",
       "       [-0.5       , -0.5       , -0.5       , -0.48823529, -0.5       ,\n",
       "        -0.30392158,  0.43333334,  0.5       ,  0.49215686,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.49215686,\n",
       "         0.5       ,  0.43333334, -0.30392158, -0.5       , -0.48823529,\n",
       "        -0.5       , -0.5       , -0.5       ],\n",
       "       [-0.5       , -0.5       , -0.48823529, -0.5       , -0.37843138,\n",
       "         0.38627452,  0.5       ,  0.49215686,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.49215686,  0.5       ,  0.38627452, -0.37843138, -0.5       ,\n",
       "        -0.48823529, -0.5       , -0.5       ],\n",
       "       [-0.5       , -0.49215686, -0.49607843, -0.43725491,  0.31176472,\n",
       "         0.5       ,  0.48823529,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48823529,  0.5       ,  0.31176472, -0.43725491,\n",
       "        -0.49607843, -0.49215686, -0.5       ],\n",
       "       [-0.49607843, -0.49215686, -0.48431373,  0.2254902 ,  0.5       ,\n",
       "         0.48431373,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.48431373,  0.5       ,  0.2254902 ,\n",
       "        -0.48431373, -0.49215686, -0.49607843],\n",
       "       [-0.48431373, -0.5       ,  0.12352941,  0.5       ,  0.48431373,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.48431373,  0.5       ,\n",
       "         0.12352941, -0.5       , -0.48431373],\n",
       "       [-0.5       ,  0.00980392,  0.5       ,  0.48431373,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.49607843,  0.49215686,  0.49215686,\n",
       "         0.49215686,  0.49215686,  0.49215686,  0.49215686,  0.49215686,\n",
       "         0.49215686,  0.49215686,  0.49215686,  0.49215686,  0.49215686,\n",
       "         0.49607843,  0.5       ,  0.5       ,  0.5       ,  0.48431373,\n",
       "         0.5       ,  0.00980392, -0.5       ],\n",
       "       [-0.06078431,  0.5       ,  0.48823529,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.48823529,  0.5       , -0.06078431],\n",
       "       [ 0.5       ,  0.49607843,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.49607843,  0.5       , -0.14313726, -0.30784315, -0.29607844,\n",
       "        -0.30784315, -0.30784315, -0.30784315, -0.30784315, -0.30784315,\n",
       "        -0.30784315, -0.30784315, -0.30784315, -0.29607844, -0.30784315,\n",
       "        -0.14313726,  0.5       ,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.49607843,  0.49607843,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.49607843,  0.5       , -0.33529413, -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.33529413,  0.5       ,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.49607843,  0.5       , -0.29215688, -0.49215686, -0.48039216,\n",
       "        -0.49215686, -0.49215686, -0.49215686, -0.49215686, -0.49215686,\n",
       "        -0.49215686, -0.49215686, -0.49215686, -0.48039216, -0.49215686,\n",
       "        -0.29215688,  0.5       ,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.49607843,  0.5       , -0.30000001, -0.5       , -0.48823529,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       , -0.48823529, -0.5       ,\n",
       "        -0.30000001,  0.5       ,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.49607843,  0.5       , -0.30000001, -0.5       , -0.48823529,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       , -0.48823529, -0.5       ,\n",
       "        -0.30000001,  0.5       ,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.49607843,  0.5       , -0.29607844, -0.49607843, -0.48431373,\n",
       "        -0.49607843, -0.49607843, -0.49607843, -0.49607843, -0.49607843,\n",
       "        -0.49607843, -0.49607843, -0.49607843, -0.48431373, -0.49607843,\n",
       "        -0.29607844,  0.5       ,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.49607843,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.49607843,  0.5       , -0.31960785, -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.31960785,  0.5       ,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.49607843,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.49607843,  0.5       , -0.22156863, -0.40196079, -0.39019608,\n",
       "        -0.40196079, -0.40196079, -0.40196079, -0.40196079, -0.40196079,\n",
       "        -0.40196079, -0.40196079, -0.40196079, -0.39019608, -0.40588236,\n",
       "        -0.2254902 ,  0.5       ,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.0372549 ,  0.5       ,  0.48823529,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.48039216,  0.47647059,  0.47647059,\n",
       "         0.47647059,  0.47647059,  0.47647059,  0.47647059,  0.47647059,\n",
       "         0.47647059,  0.47647059,  0.48039216,  0.47647059,  0.48431373,\n",
       "         0.5       ,  0.49607843,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [-0.5       ,  0.1       ,  0.5       ,  0.48431373,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.48039216,\n",
       "         0.04509804,  0.48431373,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [-0.48823529, -0.48823529,  0.21372549,  0.5       ,  0.48431373,\n",
       "         0.5       ,  0.5       ,  0.49607843,  0.49607843,  0.49607843,\n",
       "         0.49607843,  0.49607843,  0.49607843,  0.49607843,  0.49607843,\n",
       "         0.49607843,  0.49215686,  0.48823529,  0.5       , -0.16666667,\n",
       "        -0.37450981,  0.5       ,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [-0.49215686, -0.49607843, -0.44509804,  0.30392158,  0.5       ,\n",
       "         0.48823529,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.49607843,  0.48823529,  0.5       , -0.07254902, -0.5       ,\n",
       "        -0.28431374,  0.5       ,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [-0.5       , -0.49215686, -0.5       , -0.38235295,  0.37843138,\n",
       "         0.5       ,  0.49215686,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.49607843,\n",
       "         0.48431373,  0.5       , -0.04509804, -0.5       , -0.49215686,\n",
       "        -0.30000001,  0.5       ,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [-0.5       , -0.5       , -0.48823529, -0.5       , -0.31176472,\n",
       "         0.43725491,  0.49607843,  0.49215686,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.49607843,  0.48823529,\n",
       "         0.5       , -0.00196078, -0.5       , -0.47647059, -0.5       ,\n",
       "        -0.30000001,  0.5       ,  0.49607843,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [-0.5       , -0.5       , -0.5       , -0.48431373, -0.5       ,\n",
       "        -0.22156863,  0.48431373,  0.5       ,  0.49607843,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.49215686,  0.5       ,\n",
       "         0.0372549 , -0.5       , -0.48823529, -0.48823529, -0.5       ,\n",
       "        -0.30000001,  0.48823529,  0.49215686,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [-0.5       , -0.5       , -0.5       , -0.5       , -0.48431373,\n",
       "        -0.5       , -0.11960784,  0.48431373,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.07647059,\n",
       "        -0.5       , -0.48431373, -0.49607843, -0.48823529, -0.5       ,\n",
       "        -0.29607844,  0.5       ,  0.5       ,  0.48431373,  0.49607843,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [-0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.49215686, -0.5       , -0.37450981, -0.35490197, -0.35882354,\n",
       "        -0.35490197, -0.35490197, -0.35490197, -0.35490197, -0.5       ,\n",
       "        -0.49607843, -0.49607843, -0.5       , -0.49607843, -0.49607843,\n",
       "        -0.46470588, -0.00980392,  0.4254902 ,  0.5       ,  0.5       ,\n",
       "         0.48823529,  0.5       ,  0.5       ],\n",
       "       [-0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.49607843, -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.37450981,  0.08039216,  0.46862745,\n",
       "         0.5       ,  0.5       ,  0.49215686],\n",
       "       [-0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.49215686, -0.49215686, -0.49215686,\n",
       "        -0.49215686, -0.49215686, -0.49215686, -0.49215686, -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.48823529, -0.5       , -0.5       , -0.34313726,\n",
       "         0.12352941,  0.48039216,  0.5       ],\n",
       "       [-0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.49215686, -0.48823529, -0.5       ,\n",
       "        -0.5       , -0.24509804,  0.24509804]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x115500310>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1154802d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html#matplotlib.pyplot.imshow\n",
    "plt.imshow(image_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.01568628  0.\n",
      "   0.30980393  0.96862745  0.99215686  1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   0.99215686  0.96862745  0.30980393  0.          0.01568628  0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.01176471  0.          0.19607843\n",
      "   0.93333334  1.          0.99215686  1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   0.99215686  1.          0.93333334  0.19607843  0.          0.01176471\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.01176471  0.          0.12156863  0.88627452\n",
      "   1.          0.99215686  1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.          1.\n",
      "   0.99215686  1.          0.88627452  0.12156863  0.          0.01176471\n",
      "   0.          0.        ]\n",
      " [ 0.          0.00784314  0.00392157  0.0627451   0.81176472  1.\n",
      "   0.98823529  1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.          1.\n",
      "   1.          0.98823529  1.          0.81176472  0.0627451   0.00392157\n",
      "   0.00784314  0.        ]\n",
      " [ 0.00392157  0.00784314  0.01568628  0.72549021  1.          0.98431373\n",
      "   1.          1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          0.98431373  1.          0.72549021  0.01568628\n",
      "   0.00784314  0.00392157]\n",
      " [ 0.01568628  0.          0.62352943  1.          0.98431373  1.          1.\n",
      "   1.          1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          0.98431373  1.          0.62352943  0.\n",
      "   0.01568628]\n",
      " [ 0.          0.50980395  1.          0.98431373  1.          1.          1.\n",
      "   0.99607843  0.99215686  0.99215686  0.99215686  0.99215686  0.99215686\n",
      "   0.99215686  0.99215686  0.99215686  0.99215686  0.99215686  0.99215686\n",
      "   0.99215686  0.99607843  1.          1.          1.          0.98431373\n",
      "   1.          0.50980395  0.        ]\n",
      " [ 0.43921569  1.          0.98823529  1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          0.98823529  1.\n",
      "   0.43921569]\n",
      " [ 1.          0.99607843  0.99607843  1.          1.          0.99607843\n",
      "   1.          0.35686275  0.19215687  0.20392157  0.19215687  0.19215687\n",
      "   0.19215687  0.19215687  0.19215687  0.19215687  0.19215687  0.19215687\n",
      "   0.20392157  0.19215687  0.35686275  1.          0.99607843  1.          1.\n",
      "   0.99607843  0.99607843  1.        ]\n",
      " [ 1.          1.          1.          1.          1.          0.99607843\n",
      "   1.          0.16470589  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.16470589  1.          0.99607843  1.          1.          1.          1.\n",
      "   1.        ]\n",
      " [ 1.          1.          1.          1.          1.          0.99607843\n",
      "   1.          0.20784314  0.00784314  0.01960784  0.00784314  0.00784314\n",
      "   0.00784314  0.00784314  0.00784314  0.00784314  0.00784314  0.00784314\n",
      "   0.01960784  0.00784314  0.20784314  1.          0.99607843  1.          1.\n",
      "   1.          1.          1.        ]\n",
      " [ 1.          1.          1.          1.          1.          0.99607843\n",
      "   1.          0.2         0.          0.01176471  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.01176471\n",
      "   0.          0.2         1.          0.99607843  1.          1.          1.\n",
      "   1.          1.        ]\n",
      " [ 1.          1.          1.          1.          1.          0.99607843\n",
      "   1.          0.2         0.          0.01176471  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.01176471\n",
      "   0.          0.2         1.          0.99607843  1.          1.          1.\n",
      "   1.          1.        ]\n",
      " [ 1.          1.          1.          1.          1.          0.99607843\n",
      "   1.          0.20392157  0.00392157  0.01568628  0.00392157  0.00392157\n",
      "   0.00392157  0.00392157  0.00392157  0.00392157  0.00392157  0.00392157\n",
      "   0.01568628  0.00392157  0.20392157  1.          0.99607843  1.          1.\n",
      "   1.          1.          1.        ]\n",
      " [ 0.99607843  1.          1.          1.          1.          0.99607843\n",
      "   1.          0.18039216  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.18039216  1.          0.99607843  1.          1.          1.          1.\n",
      "   1.        ]\n",
      " [ 1.          0.99607843  1.          1.          1.          0.99607843\n",
      "   1.          0.27843139  0.09803922  0.10980392  0.09803922  0.09803922\n",
      "   0.09803922  0.09803922  0.09803922  0.09803922  0.09803922  0.09803922\n",
      "   0.10980392  0.09411765  0.27450982  1.          0.99607843  1.          1.\n",
      "   1.          1.          1.        ]\n",
      " [ 0.53725493  1.          0.98823529  1.          1.          1.          1.\n",
      "   0.98039216  0.97647059  0.97647059  0.97647059  0.97647059  0.97647059\n",
      "   0.97647059  0.97647059  0.97647059  0.97647059  0.98039216  0.97647059\n",
      "   0.98431373  1.          0.99607843  1.          1.          1.          1.\n",
      "   1.          1.        ]\n",
      " [ 0.          0.60000002  1.          0.98431373  1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          0.98039216\n",
      "   0.54509807  0.98431373  1.          1.          1.          1.          1.\n",
      "   1.        ]\n",
      " [ 0.01176471  0.01176471  0.71372551  1.          0.98431373  1.          1.\n",
      "   0.99607843  0.99607843  0.99607843  0.99607843  0.99607843  0.99607843\n",
      "   0.99607843  0.99607843  0.99607843  0.99215686  0.98823529  1.\n",
      "   0.33333334  0.1254902   1.          0.99607843  1.          1.          1.\n",
      "   1.          1.        ]\n",
      " [ 0.00784314  0.00392157  0.05490196  0.80392158  1.          0.98823529\n",
      "   1.          1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          0.99607843  0.98823529  1.          0.42745098\n",
      "   0.          0.21568628  1.          0.99607843  1.          1.          1.\n",
      "   1.          1.        ]\n",
      " [ 0.          0.00784314  0.          0.11764706  0.87843138  1.\n",
      "   0.99215686  1.          1.          1.          1.          1.          1.\n",
      "   1.          0.99607843  0.98431373  1.          0.45490196  0.\n",
      "   0.00784314  0.2         1.          0.99607843  1.          1.          1.\n",
      "   1.          1.        ]\n",
      " [ 0.          0.          0.01176471  0.          0.1882353   0.93725491\n",
      "   0.99607843  0.99215686  1.          1.          1.          1.          1.\n",
      "   0.99607843  0.98823529  1.          0.49803922  0.          0.02352941\n",
      "   0.          0.2         1.          0.99607843  1.          1.          1.\n",
      "   1.          1.        ]\n",
      " [ 0.          0.          0.          0.01568628  0.          0.27843139\n",
      "   0.98431373  1.          0.99607843  1.          1.          1.          1.\n",
      "   0.99215686  1.          0.53725493  0.          0.01176471  0.01176471\n",
      "   0.          0.2         0.98823529  0.99215686  1.          1.          1.\n",
      "   1.          1.        ]\n",
      " [ 0.          0.          0.          0.          0.01568628  0.\n",
      "   0.38039216  0.98431373  1.          1.          1.          1.          1.\n",
      "   1.          0.57647061  0.          0.01568628  0.00392157  0.01176471\n",
      "   0.          0.20392157  1.          1.          0.98431373  0.99607843\n",
      "   1.          1.          1.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.00784314\n",
      "   0.          0.1254902   0.14509805  0.14117648  0.14509805  0.14509805\n",
      "   0.14509805  0.14509805  0.          0.00392157  0.00392157  0.\n",
      "   0.00392157  0.00392157  0.03529412  0.49019608  0.9254902   1.          1.\n",
      "   0.98823529  1.          1.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.00392157  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.1254902   0.58039218  0.96862745  1.          1.\n",
      "   0.99215686]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.00784314  0.00784314  0.00784314  0.00784314  0.00784314  0.00784314\n",
      "   0.00784314  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.01176471  0.          0.          0.15686275  0.62352943\n",
      "   0.98039216  1.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.00784314  0.01176471  0.          0.          0.25490198\n",
      "   0.74509805]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEQBJREFUeJzt3XuMXOV5x/Hfs+u1Xduk4uo6xsVc\nbIRBjYm2DiWoBVHCtUBSQCAlcVSKoQGVSyqBSCpoI1WoIqRuRGlN4mDSBJI0ENyKNgErKkrDbbkZ\nDCSmjrlYxjYXNVxqey9P/9hDupg97zveMzPnrJ/vR7J2dp45M8+eOT/P5T3nvObuAhBPT90NAKgH\n4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/ENSUbj7YVJvm0zWzmw/5/8zS9cyejsP7lve9YO6W\n5LK9Sj+2ZeoYnyvznCXq6zfNTi7b+/o76QevuD11yna9o52+o6UNqlL4zewUScsl9Ur6urvfkLr9\ndM3Ux+zEKg85YdY3NVn3wZ3J+ptn/l5p7d4v35hc9jd7pifrfdabrGN8gz6crP/PyPbS2ml/eWVy\n2b1vezBZr7o9dcrDvqbl2074bb+Z9Uq6WdKpkhZJusDMFk30/gB0V5XP/EskveDuG9x9p6Q7JZ3V\nnrYAdFqV8M+V9PKY318prnsfM1tmZgNmNjCoHRUeDkA7dfzbfndf4e797t7fp2mdfjgALaoS/k2S\n5o35/cDiOgCTQJXwPyppgZkdbGZTJZ0vaXV72gLQaRMe6nP3ITO7TNKPNDrUt9Ld17Wts91kU9J/\nSm7oZfiEjybr37z+ptLafr3pfRdyQ1LojNTzkno+JenyFy9N1nt/8niynt0eh4aS9W6oNM7v7vdK\nurdNvQDoInbvBYIi/EBQhB8IivADQRF+ICjCDwTV1eP5q0qNnebGTXsXLUzW//zW7yTrR079jdJa\nbhyfQ3Y7I7deU89L6vmUpCsz28PNZ56ZrA8/+4tkvcq23C688gNBEX4gKMIPBEX4gaAIPxAU4QeC\natZQX0966MaHy4duemcfkFz22DvXJuunzyg/06sk7fDB0to060sui3qkhgJTz6cknT4jfd9PZLan\nn53028n68NZt5cVMDjTSnkPEeeUHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaC6P86fm9o4teiU8vH0\n6d9PT4n8pf2eT9bfHUmf2ntGT3pWVkwuuX0zcttDbnv61PfnJevvnlj++Kn9WSSlM7QbM4Pzyg8E\nRfiBoAg/EBThB4Ii/EBQhB8IivADQVUa5zezjZLekjQsacjd+zMLJMfqc9Nov/S9I0przx72z8ll\nGcfH7shtD7nt6a7D7kvWF93x6dLavHOeSS5rfYneBlvfj6YdO/mc4O6vteF+AHQRb/uBoKqG3yX9\n2MweM7Nl7WgIQHdUfdt/nLtvMrMDJN1nZs+7+wNjb1D8p7BMkqYrc2I0AF1T6ZXf3TcVP7dKulvS\nknFus8Ld+929v8+mV3k4AG004fCb2Uwz2+u9y5I+ISn9NSWAxqjytn+2pLtt9PDCKZK+4+7/0Zau\nAHTchMPv7hskfWQ3F0qO5a9ffkxy8Q3H/mNprc5x/GEfSdZHducga7RNj8rHvHut2nfdVfcDePbY\n8v1SDll+SXLZBZc/VF701rc1hvqAoAg/EBThB4Ii/EBQhB8IivADQXX11N07PzxTL118bGl9w7n/\nkFw+Na1ynYfk5oaNMhMuYw+U2x5T2/KGc8uHtCXpiDc/X1rb+U+JYcBd8MoPBEX4gaAIPxAU4QeC\nIvxAUIQfCIrwA0GZ78YhgFX1f2S6P/Kj8qmLc4fGplQ9RDP32Kn7P/pvysddJWnfZ7Yn696bPt2y\nDcc8JLjqenn9qPIzRz1xbXqfkirbQys6ta0vOfllDTy1vaXzd/PKDwRF+IGgCD8QFOEHgiL8QFCE\nHwiK8ANBdfV4/pyqY6d1OWDg7fQNHlrbnUbwPgds/526WyjVhG29/g4A1ILwA0ERfiAowg8ERfiB\noAg/EBThB4LKjvOb2UpJZ0ja6u5HFdftI+m7kuZL2ijpPHd/s3NtNtvgrL5kva8nfeZ+60s/DT44\ntNs97Qmqrpfc8xJdK6/8t0k6ZZfrrpG0xt0XSFpT/A5gEsmG390fkPTGLlefJWlVcXmVpLPb3BeA\nDpvoZ/7Z7r65uPyqpNlt6gdAl1T+ws9HTwJYejI1M1tmZgNmNrDt9eGqDwegTSYa/i1mNkeSip9b\ny27o7ivcvd/d+/fflykrgaaYaPhXS1paXF4q6Z72tAOgW7LhN7M7JD0o6XAze8XMLpR0g6STzGy9\npD8sfgcwiWTH+d39gpLSiW3uZdKy3CnYRzLfdQxnTrOeW35PVXG9ZJ+X4NjDDwiK8ANBEX4gKMIP\nBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFSjpuietDJHnsoyN8hN15xbfk9V\ndb0EXW2t4pUfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Lq6ji/yzXs5edT7s2N6zZUz47MqbW9dDaz\n0fLgzjZ2s+eoul6yz0twkzNtACoj/EBQhB8IivADQRF+ICjCDwRF+IGgsuP8ZrZS0hmStrr7UcV1\n10u6SNK24mbXuvu92fuSJcfyU/sASM3dD+CdA6cn63sfMj9Z9ym9yboNxRyvzq6XwaFk/c256ecl\nulbSdJukU8a5/qvuvrj4lw0+gGbJht/dH5D0Rhd6AdBFVd5HX2Zma81spZnt3baOAHTFRMN/i6RD\nJS2WtFnSV8puaGbLzGzAzAa2vR7zsyvQRBMKv7tvcfdhdx+RdKukJYnbrnD3fnfv33/f9Bc4ALpn\nQuE3szljfv2kpGfa0w6AbmllqO8OScdL2s/MXpF0naTjzWyxJJe0UdLFHewRQAeYZ441b6dpBx3o\nv/XFy0vrvzxzRXL5HT5Yft/WN+G+qnp7ZHuyPqzurePJpDdzYv3ceutT+mPkjJ6p5fc9SfcpyVly\n8ssaeGp7SzMWTM6/EEBlhB8IivADQRF+ICjCDwRF+IGgunrq7mkvvauFlzxSWl/woc8ll19//G2l\ntXdH0qd5Tg37VDWrh0NH65AbYj34Xy8qra07/ebksjOsc9tLU/DKDwRF+IGgCD8QFOEHgiL8QFCE\nHwiK8ANBdXWcXybZlPKHPPSz6XOCnHH/qaW1f1v478llO7kfQO7w0MhGEofljii93nKHaf/BX1+Z\nrB/xX+XnnZ3xR+nne0895Hesyf8XAJgQwg8ERfiBoAg/EBThB4Ii/EBQhB8Iqrvj/C75SOJ0zJmx\n1ZFzysfqr1tzZHLZv9p/XbKeOi24lB5z3hPGfCcqNx6eGsvPjeMvuuXzyfq8FT9L1v/35P5kPbq4\nWy0QHOEHgiL8QFCEHwiK8ANBEX4gKMIPBJUd5zezeZJulzRbkkta4e7LzWwfSd+VNF/SRknnufub\n2UccGS6v9aSnXB5+7fXS2qPnL0ou+8PVLybrZ898O1kf9PK++yzd92SWG8ff4UPJeuo8CYev/LPk\nsvO/nB7Hz20vPYNMjZ7Syiv/kKQvuPsiScdIutTMFkm6RtIad18gaU3xO4BJIht+d9/s7o8Xl9+S\n9JykuZLOkrSquNkqSWd3qkkA7bdbn/nNbL6koyU9LGm2u28uSq9q9GMBgEmi5fCb2SxJP5B0hbv/\namzN3V0a/2RtZrbMzAbMbGBQOyo1C6B9Wgq/mfVpNPjfdve7iqu3mNmcoj5H0tbxlnX3Fe7e7+79\nfZrWjp4BtEE2/GZmkr4h6Tl3v2lMabWkpcXlpZLuaX97ADqllUN6Py7pM5KeNrMni+uulXSDpO+Z\n2YWSXpR0XuVuUsOASp/2e/i59cllb/7Tc5P1w1elp2xe2Fc+DXdqGFBq9lBgbihvSOm/LXfK80Pv\nvKS0dtiXHkwu2zM9PfX5yPb0FN1Iy4bf3X8qyUrKJ7a3HQDdwh5+QFCEHwiK8ANBEX4gKMIPBEX4\ngaC6e+ruinyo/PBR60uPN/f85xPJ+oXXXJWs33/j35fWcmPdTZ7uOTWFtpQ/vfbBq5cl6wuveqi0\nltpvQ0o/31XlTtXek3ldbO6eG63jlR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgppU4/wpPlg+fbeU\n3w9grzvLx6Ml6ZgPX1Fae+iqv0sum9sPoE65cw0cct+fJOsLL3kkWU+tdx9Kj7Vbb8XR9LID0ZXf\nfyHn3ZH09jbN0tFqwrTu9XcAoBaEHwiK8ANBEX4gKMIPBEX4gaAIPxCUjc601R0fsn38Y9bQs31n\npntOzSnQe+Th6UWnpsd8LfMcuCUGrDvMn1iXvkGV3nLbXsW/u2fWrNLa+uuOTC77L3+8PFlfPK3a\n7FOp8wlU2Qdhyckva+Cp7S2tOF75gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCo7Di/mc2TdLuk2ZJc\n0gp3X25m10u6SNK24qbXuvu9qftq9Dh/TmrMuYv7SqA7cvtuPH/1zGT9kRO+lqzv15tePmXQy/c5\nOfaUTXrsqR0tjfO3cjKPIUlfcPfHzWwvSY+Z2X1F7avufmMrDwSgWbLhd/fNkjYXl98ys+ckze10\nYwA6a7c+85vZfElHS3q4uOoyM1trZivNbO+SZZaZ2YCZDQxqR6VmAbRPy+E3s1mSfiDpCnf/laRb\nJB0qabFG3xl8Zbzl3H2Fu/e7e3+fqu0PDaB9Wgq/mfVpNPjfdve7JMndt7j7sLuPSLpV0pLOtQmg\n3bLhNzOT9A1Jz7n7TWOunzPmZp+U9Ez72wPQKa182/9xSZ+R9LSZPVlcd62kC8xssUaH/zZKurgj\nHTZFajgvdzjwZJY4lHkyy00PPrzu58n6gs+m7//Tv3tJsv7S1eXb0yPHfD257Kye6aU1S52vfBet\nfNv/U41/BvTkmD6AZmMPPyAowg8ERfiBoAg/EBThB4Ii/EBQe8wU3bXaQ8fC92Q+NJS+QWbfDetJ\nj6f7o08n6/POKa+d+qnLk8se9Bfl+yD8cucPk8uOxSs/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTV\n1Sm6zWybpBfHXLWfpNe61sDuaWpvTe1LoreJamdvB7n7/q3csKvh/8CDmw24e39tDSQ0tbem9iXR\n20TV1Rtv+4GgCD8QVN3hX1Hz46c0tbem9iXR20TV0lutn/kB1KfuV34ANakl/GZ2ipn93MxeMLNr\n6uihjJltNLOnzexJMxuouZeVZrbVzJ4Zc90+Znafma0vfo47TVpNvV1vZpuKdfekmZ1WU2/zzOwn\nZvasma0zs8uL62tdd4m+allvXX/bb2a9kn4h6SRJr0h6VNIF7v5sVxspYWYbJfW7e+1jwmb2+5Le\nlnS7ux9VXPe3kt5w9xuK/zj3dverG9Lb9ZLernvm5mJCmTljZ5aWdLakz6nGdZfo6zzVsN7qeOVf\nIukFd9/g7jsl3SnprBr6aDx3f0DSG7tcfZakVcXlVRrdeLqupLdGcPfN7v54cfktSe/NLF3rukv0\nVYs6wj9X0stjfn9FzZry2yX92MweM7NldTczjtnFtOmS9Kqk2XU2M47szM3dtMvM0o1ZdxOZ8brd\n+MLvg45z949KOlXSpcXb20by0c9sTRquaWnm5m4ZZ2bpX6tz3U10xut2qyP8myTNG/P7gcV1jeDu\nm4qfWyXdrebNPrzlvUlSi59ba+7n15o0c/N4M0urAeuuSTNe1xH+RyUtMLODzWyqpPMlra6hjw8w\ns5nFFzEys5mSPqHmzT68WtLS4vJSSffU2Mv7NGXm5rKZpVXzumvcjNfu3vV/kk7T6Df+/y3pi3X0\nUNLXIZKeKv6tq7s3SXdo9G3goEa/G7lQ0r6S1khaL+l+Sfs0qLdvSXpa0lqNBm1OTb0dp9G39Gsl\nPVn8O63udZfoq5b1xh5+QFB84QcERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+IKj/A5IeSPbuoRKE\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1154d8350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "# matplotlib.image.imread get data as [0, 1]\n",
    "img=mpimg.imread('./notMNIST_large/A/a29ydW5pc2hpLnR0Zg==.png')\n",
    "print(img)\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABUElEQVR4nH2SvUpDQRCFz+yuIgQk\nYKONSSmpJBGsrSxT2dn4AGKK9EI6sdTGKo3PIKTxDSSdjY2kFWIZcvfnWNydm2uCftWw3w47e3YB\nAA79RWRFXPThkLHozlmXnHdh1bVmdUdGzlqlNdKcMvAXgdOmGECMm9BzDc+JMwKH8aYjPcdwwIie\nZIo1EknPETBYP2917kBICCiX7zbqzWLnmQJCQJJM7KBGh4kkqUk0TK2zkSuVKUnKpSStDP5BO0VE\nVqVKllXBoPsDCpVS2oOv1UBhX/fdlSEUS6VY+nwTAk8b0Rb3eQXWvtCTSYmBF+3cCiO7b7V4U+A1\nTlXC4PCz+gjJcwjTqyQsjr9T1L5bbKMm4XCeB4y8g7XohVhJOFzl+R7gxOKE9InM+ZjU3qIA+BBQ\nsDO82UO0GoWmqbQfF6RGBlOiCQJH44J/PJaxwNnrD42Fhlk7sTnAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(filename='./notMNIST_large/A/a29ydW5pc2hpLnR0Zg==.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYznx5jUwzoO"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Another check: we expect the data to be balanced across classes. Verify that.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./notMNIST_large/A.pickle (52909, 28, 28)\n",
      "./notMNIST_large/B.pickle (52911, 28, 28)\n",
      "./notMNIST_large/C.pickle (52912, 28, 28)\n",
      "./notMNIST_large/D.pickle (52911, 28, 28)\n",
      "./notMNIST_large/E.pickle (52912, 28, 28)\n",
      "./notMNIST_large/F.pickle (52912, 28, 28)\n",
      "./notMNIST_large/G.pickle (52912, 28, 28)\n",
      "./notMNIST_large/H.pickle (52912, 28, 28)\n",
      "./notMNIST_large/I.pickle (52912, 28, 28)\n",
      "./notMNIST_large/J.pickle (52911, 28, 28)\n",
      "./notMNIST_small/A.pickle (1872, 28, 28)\n",
      "./notMNIST_small/B.pickle (1873, 28, 28)\n",
      "./notMNIST_small/C.pickle (1873, 28, 28)\n",
      "./notMNIST_small/D.pickle (1873, 28, 28)\n",
      "./notMNIST_small/E.pickle (1873, 28, 28)\n",
      "./notMNIST_small/F.pickle (1872, 28, 28)\n",
      "./notMNIST_small/G.pickle (1872, 28, 28)\n",
      "./notMNIST_small/H.pickle (1872, 28, 28)\n",
      "./notMNIST_small/I.pickle (1872, 28, 28)\n",
      "./notMNIST_small/J.pickle (1872, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "for dataset_file in train_datasets + test_datasets:\n",
    "    with open(dataset_file, 'rb') as f:\n",
    "        dataset_tmp = pickle.load(f)        \n",
    "        print(dataset_file, dataset_tmp.shape)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LA7M7K22ynCt"
   },
   "source": [
    "Merge and prune the training data as needed. Depending on your computer setup, you might not be able to fit it all in memory, and you can tune `train_size` as needed. The labels will be stored into a separate array of integers 0 through 9.\n",
    "\n",
    "Also create a validation dataset for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 411281,
     "status": "ok",
     "timestamp": 1444485897869,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "s3mWgZLpyuzq",
    "outputId": "8af66da6-902d-4719-bedc-7c9fb7ae7948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes  # integer division\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class + tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "# can tune the sizes here\n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPTCnjIcyuKN"
   },
   "source": [
    "Next, we'll randomize the data. It's important to have the labels well shuffled for the training and test distributions to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6WZ2l2tN2zOL"
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "\n",
    "# avoid sequential training data, like all As, then all Bs, then all Cs, etc.\n",
    "# That breaks the assumption that your data is taking random steps over time.\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puDUTe6t6USl"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "Convince yourself that the data is still good after shuffling!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11561e790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_tmp = train_dataset[0]\n",
    "plt.imshow(image_tmp)\n",
    "\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIQJaJuwg5Hw"
   },
   "source": [
    "Finally, let's save the data for later reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QiR_rETzem6C"
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 413065,
     "status": "ok",
     "timestamp": 1444485899688,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "hQbLjrW_iT39",
    "outputId": "b440efc6-5ee1-4cbc-d02d-93db44ebd956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800441\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gE_cRAQB33lk"
   },
   "source": [
    "---\n",
    "Problem 5\n",
    "---------\n",
    "\n",
    "By construction, this dataset might contain a lot of overlapping samples, including training data that's also contained in the validation and test set! Overlap between training and test can skew the results if you expect to use your model in an environment where there is never an overlap, but are actually ok if you expect to see training samples recur when you use it.\n",
    "Measure how much overlap there is between training, validation and test samples.\n",
    "\n",
    "Optional questions:\n",
    "- What about near duplicates between datasets? (images that are almost identical)\n",
    "- Create a sanitized validation and test set, and compare your accuracy on those in subsequent assignments.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 in [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2]\n",
      " [3 5]]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2], [3, 4]])\n",
    "print(a)\n",
    "\n",
    "b = np.array([[1,2], [3, 5]])\n",
    "print(b)\n",
    "\n",
    "print(np.array_equal(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal([1,2], [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose([1,2], [1, 2.0000], equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_duplicate(dataset1, dataset2):\n",
    "#     dataset1_loop = 0\n",
    "#     dataset2_loop = 0\n",
    "#     dup_count = 0\n",
    "    \n",
    "#     for i in dataset1:\n",
    "#         dataset1_loop += 1\n",
    "#         if (dataset1_loop % 100) == 0:\n",
    "#             print('dataset1_loop', dataset1_loop)\n",
    "            \n",
    "#         for j in dataset2:\n",
    "#             dataset2_loop += 1\n",
    "#             if np.array_equal(i, j):\n",
    "#                 dup_count += 1\n",
    "#                 print('dup', dup_count)\n",
    "\n",
    "#     print(dataset1.shape, dataset2.shape, dup_count)\n",
    "\n",
    "    \n",
    "# check_duplicate(test_dataset, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L8oww1s4JMQx"
   },
   "source": [
    "---\n",
    "Problem 6\n",
    "---------\n",
    "\n",
    "Let's get an idea of what an off-the-shelf classifier can give you on this data. It's always good to check that there is something to learn, and that it's a problem that is not so trivial that a canned solution solves it.\n",
    "\n",
    "Train a simple model on this data using 50, 100, 1000 and 5000 training samples. Hint: you can use the LogisticRegression model from sklearn.linear_model.\n",
    "\n",
    "Optional question: train an off-the-shelf model on all the data!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_shape (200000, 28, 28) new_shape (200000, 784)\n",
      "old_shape (10000, 28, 28) new_shape (10000, 784)\n",
      "old_shape (10000, 28, 28) new_shape (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "def flat_dataset(dataset):        \n",
    "    d_shape = dataset.shape\n",
    "    new_dataset = dataset.reshape(d_shape[0], d_shape[1] * d_shape[2])\n",
    "    print('old_shape', d_shape, 'new_shape', new_dataset.shape)\n",
    "    return new_dataset\n",
    "\n",
    "train_data = flat_dataset(train_dataset)\n",
    "valid_data = flat_dataset(valid_dataset)\n",
    "test_data = flat_dataset(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=-1, penalty='l2', random_state=42, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_samples = 200000\n",
    "\n",
    "clf = LogisticRegression(random_state=42, solver='sag', multi_class='multinomial', n_jobs=-1)\n",
    "clf.fit(train_data[:num_train_samples], train_labels[:num_train_samples])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 9, 3, ..., 8, 9, 8], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_pred = clf.predict(valid_data)\n",
    "valid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8313\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.84      0.85      1000\n",
      "          1       0.84      0.81      0.82      1000\n",
      "          2       0.86      0.87      0.87      1000\n",
      "          3       0.85      0.85      0.85      1000\n",
      "          4       0.83      0.78      0.80      1000\n",
      "          5       0.84      0.88      0.86      1000\n",
      "          6       0.84      0.83      0.83      1000\n",
      "          7       0.82      0.83      0.82      1000\n",
      "          8       0.75      0.79      0.77      1000\n",
      "          9       0.83      0.83      0.83      1000\n",
      "\n",
      "avg / total       0.83      0.83      0.83     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 200,000 samples, sag, multinomial\n",
    "print(clf.score(valid_data, valid_labels))\n",
    "print(classification_report(valid_labels, valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6, 4, ..., 9, 9, 7], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = clf.predict(test_data)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.61      0.75      1000\n",
      "          1       0.75      0.71      0.73      1000\n",
      "          2       0.43      0.96      0.60      1000\n",
      "          3       0.57      0.83      0.68      1000\n",
      "          4       0.72      0.10      0.17      1000\n",
      "          5       0.56      0.78      0.65      1000\n",
      "          6       0.92      0.41      0.57      1000\n",
      "          7       0.87      0.72      0.79      1000\n",
      "          8       0.78      0.56      0.65      1000\n",
      "          9       0.64      0.77      0.70      1000\n",
      "\n",
      "avg / total       0.72      0.65      0.63     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 50, sag, multinomial\n",
    "print(classification_report(test_labels, test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.80      0.83      1000\n",
      "          1       0.84      0.83      0.83      1000\n",
      "          2       0.78      0.88      0.83      1000\n",
      "          3       0.84      0.88      0.86      1000\n",
      "          4       0.82      0.76      0.79      1000\n",
      "          5       0.88      0.85      0.87      1000\n",
      "          6       0.90      0.79      0.84      1000\n",
      "          7       0.87      0.83      0.85      1000\n",
      "          8       0.71      0.82      0.76      1000\n",
      "          9       0.80      0.84      0.82      1000\n",
      "\n",
      "avg / total       0.83      0.83      0.83     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1000, sag, ovr\n",
    "print(classification_report(test_labels, test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.81      0.83      1000\n",
      "          1       0.86      0.83      0.84      1000\n",
      "          2       0.80      0.88      0.84      1000\n",
      "          3       0.85      0.88      0.86      1000\n",
      "          4       0.81      0.79      0.80      1000\n",
      "          5       0.89      0.82      0.85      1000\n",
      "          6       0.91      0.79      0.85      1000\n",
      "          7       0.87      0.84      0.85      1000\n",
      "          8       0.70      0.81      0.75      1000\n",
      "          9       0.79      0.83      0.81      1000\n",
      "\n",
      "avg / total       0.83      0.83      0.83     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1000, sag, multinomial\n",
    "print(classification_report(test_labels, test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.84      0.86      1000\n",
      "          1       0.87      0.82      0.84      1000\n",
      "          2       0.84      0.88      0.86      1000\n",
      "          3       0.86      0.88      0.87      1000\n",
      "          4       0.82      0.80      0.81      1000\n",
      "          5       0.86      0.87      0.87      1000\n",
      "          6       0.87      0.85      0.86      1000\n",
      "          7       0.86      0.85      0.86      1000\n",
      "          8       0.82      0.84      0.83      1000\n",
      "          9       0.82      0.88      0.85      1000\n",
      "\n",
      "avg / total       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5000, liblinear, ovr\n",
    "print(classification_report(test_labels, test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.84      0.86      1000\n",
      "          1       0.87      0.82      0.84      1000\n",
      "          2       0.84      0.88      0.86      1000\n",
      "          3       0.86      0.87      0.86      1000\n",
      "          4       0.83      0.80      0.81      1000\n",
      "          5       0.86      0.87      0.87      1000\n",
      "          6       0.87      0.84      0.86      1000\n",
      "          7       0.86      0.86      0.86      1000\n",
      "          8       0.82      0.84      0.83      1000\n",
      "          9       0.83      0.87      0.85      1000\n",
      "\n",
      "avg / total       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5000, sag, ovr\n",
    "print(classification_report(test_labels, test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.83      0.85      1000\n",
      "          1       0.87      0.82      0.84      1000\n",
      "          2       0.84      0.88      0.86      1000\n",
      "          3       0.86      0.87      0.87      1000\n",
      "          4       0.82      0.80      0.81      1000\n",
      "          5       0.84      0.85      0.85      1000\n",
      "          6       0.84      0.83      0.84      1000\n",
      "          7       0.86      0.85      0.86      1000\n",
      "          8       0.80      0.84      0.82      1000\n",
      "          9       0.83      0.88      0.85      1000\n",
      "\n",
      "avg / total       0.84      0.84      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5000, sag, multinomial\n",
    "print(classification_report(test_labels, test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9006\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.91      0.91      1000\n",
      "          1       0.91      0.90      0.90      1000\n",
      "          2       0.89      0.93      0.91      1000\n",
      "          3       0.92      0.92      0.92      1000\n",
      "          4       0.92      0.86      0.89      1000\n",
      "          5       0.90      0.93      0.91      1000\n",
      "          6       0.92      0.90      0.91      1000\n",
      "          7       0.92      0.90      0.91      1000\n",
      "          8       0.85      0.85      0.85      1000\n",
      "          9       0.87      0.92      0.89      1000\n",
      "\n",
      "avg / total       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 200,000 samples, sag, multinomial\n",
    "print(clf.score(test_data, test_labels))\n",
    "print(classification_report(test_labels, test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
